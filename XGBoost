# -*- coding: utf-8 -*-
"""
XGBoost regression training (baseline "original code") with early stopping.
- input: CSV/Excel
- output: metrics + model + predictions

Usage:
  python xgb_train.py --input data.xlsx --target AGBD --out_dir outputs/xgb
"""

import os
import argparse
import json
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

from xgboost import XGBRegressor

try:
    import joblib
except Exception:
    joblib = None


def read_table(path: str) -> pd.DataFrame:
    ext = os.path.splitext(path)[1].lower()
    if ext in [".xlsx", ".xls"]:
        return pd.read_excel(path)
    return pd.read_csv(path, encoding="utf-8", engine="python")


def rmse(y_true, y_pred) -> float:
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", required=True)
    ap.add_argument("--target", required=True)
    ap.add_argument("--test_size", type=float, default=0.2)
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--out_dir", default="xgb_outputs")
    ap.add_argument("--tune", action="store_true", help="RandomizedSearchCV (no early stopping during tuning)")
    ap.add_argument("--early_stopping_rounds", type=int, default=50)
    ap.add_argument("--eval_size", type=float, default=0.2, help="validation split inside training for early stopping")
    args = ap.parse_args()

    os.makedirs(args.out_dir, exist_ok=True)

    df = read_table(args.input)
    if args.target not in df.columns:
        raise ValueError(f"Target '{args.target}' not found. Columns: {df.columns.tolist()}")

    y = pd.to_numeric(df[args.target], errors="coerce")
    X = df.drop(columns=[args.target]).apply(pd.to_numeric, errors="coerce")

    valid = np.isfinite(y.to_numpy())
    X = X.loc[valid].copy()
    y = y.loc[valid].copy()

    if len(X) < 30:
        raise ValueError(f"Too few samples after cleaning: {len(X)}")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=args.test_size, random_state=args.seed
    )

    # base model (keep it close to your Table 4 style)
    xgb = XGBRegressor(
        objective="reg:squarederror",
        n_estimators=2000,         # large, early stopping will find optimal
        learning_rate=0.01,
        max_depth=3,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_lambda=1.0,
        random_state=args.seed,
        n_jobs=-1
    )

    pipe = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("model", xgb)
    ])

    best_estimator = pipe

    if args.tune:
        # NOTE: early stopping cannot be directly used inside sklearn CV easily in a clean way.
        # For tuning, we do standard CV by R2. Final fit will still use early stopping.
        param_dist = {
            "model__learning_rate": [0.005, 0.01, 0.02, 0.05],
            "model__max_depth": [2, 3, 4, 5],
            "model__subsample": [0.6, 0.8, 1.0],
            "model__colsample_bytree": [0.6, 0.8, 1.0],
            "model__min_child_weight": [1, 3, 5, 10],
            "model__reg_lambda": [0.0, 0.5, 1.0, 2.0, 5.0],
            "model__reg_alpha": [0.0, 0.1, 0.5, 1.0],
        }
        search = RandomizedSearchCV(
            estimator=pipe,
            param_distributions=param_dist,
            n_iter=40,
            scoring="r2",
            cv=5,
            random_state=args.seed,
            n_jobs=-1,
            verbose=1
        )
        search.fit(X_train, y_train)
        best_estimator = search.best_estimator_

        with open(os.path.join(args.out_dir, "best_params.json"), "w", encoding="utf-8") as f:
            json.dump(search.best_params_, f, indent=2)

        print("[TUNING] best R2 (cv):", search.best_score_)
        print("[TUNING] best params saved to best_params.json")

    # ---- Final fit with early stopping ----
    # Split train -> (train_fit, train_eval)
    X_fit, X_eval, y_fit, y_eval = train_test_split(
        X_train, y_train, test_size=args.eval_size, random_state=args.seed
    )

    # Fit imputer first
    imputer = best_estimator.named_steps["imputer"]
    model = best_estimator.named_steps["model"]

    X_fit_i = imputer.fit_transform(X_fit)
    X_eval_i = imputer.transform(X_eval)
    X_test_i = imputer.transform(X_test)

    model.fit(
        X_fit_i, y_fit,
        eval_set=[(X_eval_i, y_eval)],
        verbose=False,
        early_stopping_rounds=args.early_stopping_rounds
    )

    # Predict
    pred = model.predict(X_test_i)
    r2 = float(r2_score(y_test, pred))
    mae = float(mean_absolute_error(y_test, pred))
    rmse_val = rmse(y_test, pred)

    metrics = {
        "R2": r2, "MAE": mae, "RMSE": rmse_val,
        "best_iteration": int(getattr(model, "best_iteration", -1)),
        "n_train": int(len(X_train)), "n_test": int(len(X_test))
    }
    print("[TEST]", metrics)

    with open(os.path.join(args.out_dir, "metrics.json"), "w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2)

    pd.DataFrame({"y_true": y_test.to_numpy(), "y_pred": pred}).to_csv(
        os.path.join(args.out_dir, "predictions_test.csv"),
        index=False, encoding="utf-8-sig"
    )

    # save (imputer + model)
    if joblib is not None:
        joblib.dump({"imputer": imputer, "model": model}, os.path.join(args.out_dir, "xgb_bundle.joblib"))
        print("[OK] saved: xgb_bundle.joblib")
    else:
        print("[WARN] joblib not available, skip model saving.")


if __name__ == "__main__":
    main()
